#!/usr/bin/env python3
"""
æ£€æŸ¥å¹¶ç”ŸæˆæŒ‡å®š Episode èŒƒå›´çš„éŸ³é¢‘æ–‡ä»¶è„šæœ¬

åŠŸèƒ½ï¼š
1. ä» prod_lingohow-sentences-20251113.json è¯»å–æ•°æ®
2. æ ¹æ®å‘½ä»¤è¡Œå‚æ•°ç­›é€‰æŒ‡å®šèŒƒå›´çš„ episode å¥å­
3. æ£€æŸ¥æ¯ä¸ªå¥å­çš„éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº R2 å’Œ COS
4. å°†ç¼ºå¤±éŸ³é¢‘çš„å¥å­ä¿å­˜åˆ° JSON æ–‡ä»¶
5. ä½¿ç”¨ edge-tts ç”Ÿæˆç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶
6. ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶åˆ° R2 å’Œ COS

æ€§èƒ½ä¼˜åŒ–ï¼š
- æ–‡ä»¶æ£€æŸ¥ï¼š4 å¹¶å‘
- éŸ³é¢‘ç”Ÿæˆï¼š3 å¹¶å‘
- R2 ä¸Šä¼ ï¼š2 å¹¶å‘
- COS ä¸Šä¼ ï¼š3 çº¿ç¨‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python check_and_generate_audio.py -s 238 -e 300
    python check_and_generate_audio.py --start 1 --end 100
"""

import asyncio
import json
import logging
import os
import hashlib
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime

# å¯¼å…¥ç°æœ‰çš„æœåŠ¡
try:
    from dotenv import load_dotenv
    load_dotenv(".env.local")
except ImportError:
    pass

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_sentence_hash(text: str) -> str:
    """
    ç”Ÿæˆå¥å­çš„å“ˆå¸Œå€¼ï¼ˆä¸ NextJS getAudioFileName é€»è¾‘ä¸€è‡´ï¼‰

    Args:
        text: å¥å­æ–‡æœ¬

    Returns:
        MD5 å“ˆå¸Œå€¼ï¼ˆ8ä½ï¼‰
    """
    return hashlib.md5(text.strip().encode()).hexdigest()[:8]


async def get_all_audio_files_from_r2() -> set:
    """
    ä» R2 è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆæ‰¹é‡ï¼‰

    Returns:
        åŒ…å«æ‰€æœ‰æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰çš„ Set
    """
    try:
        import aioboto3
        import os

        r2_config = {
            'bucket': os.getenv('R2_BUCKET_NAME'),
            'access_key_id': os.getenv('R2_ACCESS_KEY_ID'),
            'secret_access_key': os.getenv('R2_SECRET_ACCESS_KEY'),
            'endpoint_url': os.getenv('R2_ENDPOINT_URL')
        }

        if not all(r2_config.values()):
            logger.warning("R2 configuration incomplete, skipping")
            return set()

        existing_files = set()
        session = aioboto3.Session()

        async with session.client(
            service_name='s3',
            endpoint_url=r2_config['endpoint_url'],
            aws_access_key_id=r2_config['access_key_id'],
            aws_secret_access_key=r2_config['secret_access_key'],
            region_name='auto'
        ) as s3_client:
            continuation_token = None
            page_count = 0

            while True:
                list_params = {
                    'Bucket': r2_config['bucket'],
                    'Prefix': 'audio/sentences/',
                    'MaxKeys': 1000
                }

                if continuation_token:
                    list_params['ContinuationToken'] = continuation_token

                response = await s3_client.list_objects_v2(**list_params)
                page_count += 1

                # æå–æ–‡ä»¶å
                if 'Contents' in response:
                    for obj in response['Contents']:
                        key = obj.get('Key', '')
                        # å»æ‰ 'audio/sentences/' å‰ç¼€å’Œ '.mp3' åç¼€
                        filename = key.replace('audio/sentences/', '').replace('.mp3', '')
                        if filename:
                            existing_files.add(filename)

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
                if response.get('IsTruncated'):
                    continuation_token = response.get('NextContinuationToken')
                else:
                    break

            logger.info(f"âœ… R2: åŠ è½½äº† {len(existing_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆ{page_count} é¡µï¼‰")
            return existing_files

    except ImportError:
        logger.warning("aioboto3 æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥ R2")
        return set()
    except Exception as e:
        logger.error(f"ä» R2 è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return set()


def _get_cos_files_sync() -> set:
    """
    ä» COS è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

    Returns:
        åŒ…å«æ‰€æœ‰æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰çš„ Set
    """
    try:
        from qcloud_cos import CosConfig, CosS3Client
        import os

        cos_config = {
            'secret_id': os.getenv('COS_SECRET_ID'),
            'secret_key': os.getenv('COS_SECRET_KEY'),
            'bucket': os.getenv('COS_BUCKET'),
            'region': os.getenv('COS_REGION')
        }

        if not all(cos_config.values()):
            logger.warning("COS configuration incomplete, skipping")
            return set()

        config = CosConfig(
            Region=cos_config['region'],
            SecretId=cos_config['secret_id'],
            SecretKey=cos_config['secret_key']
        )
        client = CosS3Client(config)

        existing_files = set()
        marker = ''
        page_count = 0

        while True:
            response = client.list_objects(
                Bucket=cos_config['bucket'],
                Prefix='audio/sentences/',
                Marker=marker,
                MaxKeys=1000
            )
            page_count += 1

            # æå–æ–‡ä»¶å
            if 'Contents' in response:
                for obj in response['Contents']:
                    key = obj.get('Key', '')
                    # å»æ‰ 'audio/sentences/' å‰ç¼€å’Œ '.mp3' åç¼€
                    filename = key.replace('audio/sentences/', '').replace('.mp3', '')
                    if filename:
                        existing_files.add(filename)

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
            if response.get('IsTruncated') == 'true':
                marker = response.get('NextMarker', '')
            else:
                break

        logger.info(f"âœ… COS: åŠ è½½äº† {len(existing_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆ{page_count} é¡µï¼‰")
        return existing_files

    except ImportError:
        logger.warning("qcloud_cos æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥ COS")
        return set()
    except Exception as e:
        logger.error(f"ä» COS è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return set()


async def get_all_audio_files_from_cos() -> set:
    """
    ä» COS è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆå¼‚æ­¥åŒ…è£…å™¨ï¼‰

    Returns:
        åŒ…å«æ‰€æœ‰æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰çš„ Set
    """
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, _get_cos_files_sync)


async def check_all_sentences(
    sentences: List[Dict[str, Any]],
    max_concurrent_checks: int = 4
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æ£€æŸ¥æ‰€æœ‰å¥å­çš„éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        sentences: å¥å­åˆ—è¡¨
        max_concurrent_checks: å‚æ•°ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼ˆæ‰¹é‡æ£€æŸ¥ä¸éœ€è¦å¹¶å‘æ§åˆ¶ï¼‰

    Returns:
        (all_results, missing_sentences) å…ƒç»„
        - all_results: æ‰€æœ‰å¥å­çš„æ£€æŸ¥ç»“æœ
        - missing_sentences: ç¼ºå¤±éŸ³é¢‘çš„å¥å­åˆ—è¡¨
    """
    logger.info(f"å¼€å§‹æ£€æŸ¥ {len(sentences)} ä¸ªå¥å­çš„éŸ³é¢‘æ–‡ä»¶...")
    start_time = time.time()

    # æ­¥éª¤1: æ‰¹é‡è·å– R2 å’Œ COS çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
    logger.info("ğŸ“¥ æ­£åœ¨ä» R2 å’Œ COS æ‰¹é‡è·å–æ–‡ä»¶åˆ—è¡¨...")
    r2_files, cos_files = await asyncio.gather(
        get_all_audio_files_from_r2(),
        get_all_audio_files_from_cos()
    )

    logger.info(f"   R2 æ–‡ä»¶æ•°: {len(r2_files)}")
    logger.info(f"   COS æ–‡ä»¶æ•°: {len(cos_files)}")

    # æ­¥éª¤2: æ£€æŸ¥æ¯ä¸ªå¥å­
    all_results = []
    missing_audio_sentences = []

    logger.info("ğŸ” å¼€å§‹æ£€æŸ¥å¥å­éŸ³é¢‘çŠ¶æ€...")
    for idx, sentence in enumerate(sentences):
        en = sentence.get('en', '').strip()
        if not en:
            continue

        # ç”Ÿæˆå“ˆå¸Œ
        sentence_hash = generate_sentence_hash(en)

        # åœ¨ Set ä¸­å¿«é€ŸæŸ¥æ‰¾
        r2_exists = sentence_hash in r2_files
        cos_exists = sentence_hash in cos_files

        # åˆ›å»ºç»“æœå¯¹è±¡
        result = sentence.copy()
        result['sentence_hash'] = sentence_hash
        result['r2_exists'] = r2_exists
        result['cos_exists'] = cos_exists
        result['audio_exists'] = r2_exists and cos_exists

        all_results.append(result)

        # å¦‚æœä»»ä¸€å­˜å‚¨ä¸å­˜åœ¨ï¼Œåˆ™éœ€è¦ç”Ÿæˆ
        if not result['audio_exists']:
            missing_audio_sentences.append(result)

        # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯100ä¸ªæˆ–æœ€åä¸€ä¸ªï¼‰
        if (idx + 1) % 100 == 0 or (idx + 1) == len(sentences):
            logger.info(
                f"æ£€æŸ¥è¿›åº¦: {idx + 1}/{len(sentences)} "
                f"({(idx + 1) * 100 / len(sentences):.1f}%)"
            )

    total_time = time.time() - start_time
    logger.info(f"âœ… æ£€æŸ¥å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}ç§’")
    logger.info(f"   - æ£€æŸ¥å¥å­æ•°: {len(all_results)}")
    logger.info(f"   - éŸ³é¢‘å·²å­˜åœ¨: {len(all_results) - len(missing_audio_sentences)}")
    logger.info(f"   - éŸ³é¢‘ç¼ºå¤±: {len(missing_audio_sentences)}")

    return all_results, missing_audio_sentences


async def generate_and_upload_audio(
    sentences: List[Dict[str, Any]],
    max_concurrent_audio: int = 8,
    max_concurrent_r2: int = 20,
    max_workers_cos: int = 8
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå¹¶ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥æœ¬åœ°æ–‡ä»¶ï¼Œæœ‰åˆ™ç›´æ¥ä¸Šä¼ ï¼‰

    Args:
        sentences: éœ€è¦ç”ŸæˆéŸ³é¢‘çš„å¥å­åˆ—è¡¨
        max_concurrent_audio: éŸ³é¢‘ç”Ÿæˆæœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š8ï¼‰
        max_concurrent_r2: R2 ä¸Šä¼ æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰
        max_workers_cos: COS ä¸Šä¼ æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š8ï¼‰

    Returns:
        ç»Ÿè®¡ç»“æœ
    """
    if not sentences:
        logger.info("æ²¡æœ‰éœ€è¦ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶")
        return {
            'total': 0,
            'generated': 0,
            'uploaded_r2': 0,
            'uploaded_cos': 0
        }

    from utils.audio_generator import generate_batch_audio, check_edge_tts_available
    from services.storage_service import upload_audio_files

    # æ£€æŸ¥ edge-tts æ˜¯å¦å¯ç”¨
    if not check_edge_tts_available():
        raise Exception("edge-tts æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install edge-tts")

    # åˆ›å»ºéŸ³é¢‘ç›®å½•
    audio_dir = Path("audio/sentences")
    audio_dir.mkdir(parents=True, exist_ok=True)

    # æ­¥éª¤1: æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    logger.info(f"ğŸ” æ£€æŸ¥æœ¬åœ° audio/sentences/ æ–‡ä»¶å¤¹...")
    local_files = []
    need_generate = []

    for sentence in sentences:
        en = sentence.get('en', '').strip()
        if not en:
            continue

        sentence_hash = generate_sentence_hash(en)
        audio_path = audio_dir / f"{sentence_hash}.mp3"

        if audio_path.exists() and audio_path.stat().st_size > 0:
            # æœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥åŠ å…¥ä¸Šä¼ åˆ—è¡¨
            local_files.append({
                'en': en,
                'sentence_hash': sentence_hash,
                'audio_path': str(audio_path)
            })
        else:
            # éœ€è¦ç”Ÿæˆ
            need_generate.append(en)

    logger.info(f"   âœ… æœ¬åœ°å·²å­˜åœ¨: {len(local_files)} ä¸ªæ–‡ä»¶")
    logger.info(f"   ğŸµ éœ€è¦ç”Ÿæˆ: {len(need_generate)} ä¸ªæ–‡ä»¶")

    # æ­¥éª¤2: ç”Ÿæˆç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶
    audio_gen_time = 0
    newly_generated = []
    processed_sentences = []  # åˆå§‹åŒ–ï¼Œé¿å…UnboundLocalError

    if need_generate:
        logger.info(f"å¼€å§‹ç”Ÿæˆ {len(need_generate)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆå¹¶å‘æ•°ï¼š{max_concurrent_audio}ï¼‰...")
        audio_gen_start = time.time()

        # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        processed_sentences = await generate_batch_audio(
            sentences=need_generate,
            audio_dir=audio_dir,
            voice="en-US-AvaMultilingualNeural",
            max_concurrent=max_concurrent_audio,
            timeout_per_sentence=30
        )

        audio_gen_time = time.time() - audio_gen_start

        # ç»Ÿè®¡æ–°ç”Ÿæˆçš„æ–‡ä»¶
        newly_generated = [p for p in processed_sentences if p.get('audio_generated') and not p.get('existed')]

        logger.info(f"âœ… éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {audio_gen_time:.1f}ç§’")
        logger.info(f"   - æ–°ç”Ÿæˆ: {len(newly_generated)} ä¸ªæ–‡ä»¶")
        logger.info(f"   - ç”Ÿæˆé€Ÿåº¦: {len(need_generate) / audio_gen_time:.2f} å¥/ç§’")

        # å°†æ–°ç”Ÿæˆçš„æ–‡ä»¶æ·»åŠ åˆ°æœ¬åœ°æ–‡ä»¶åˆ—è¡¨
        for processed in processed_sentences:
            if processed.get('audio_generated') and processed.get('audio_path'):
                local_files.append({
                    'en': processed.get('en', ''),
                    'sentence_hash': processed['sentence_hash'],
                    'audio_path': processed['audio_path']
                })
    else:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²åœ¨æœ¬åœ°å­˜åœ¨ï¼Œæ— éœ€ç”Ÿæˆ")

    # æ­¥éª¤3: å‡†å¤‡ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ï¼ˆæ ¹æ®ç¼ºå¤±æƒ…å†µåˆ†åˆ«ä¸Šä¼ ï¼‰
    # ä¸ºæ¯ä¸ªå¥å­åŒ¹é…åŸå§‹æ£€æŸ¥ç»“æœï¼Œç¡®å®šéœ€è¦ä¸Šä¼ åˆ°å“ªä¸ªå­˜å‚¨
    upload_files_r2 = []
    upload_files_cos = []

    # åˆ›å»ºå¥å­hashåˆ°æ£€æŸ¥ç»“æœçš„æ˜ å°„
    sentence_check_map = {s.get('sentence_hash'): s for s in sentences}

    for file_info in local_files:
        audio_path = file_info['audio_path']
        sentence_hash = file_info['sentence_hash']

        if Path(audio_path).exists():
            object_key = f"audio/sentences/{sentence_hash}.mp3"
            file_data = {
                'file_path': audio_path,
                'object_key': object_key,
                'sentence_hash': sentence_hash
            }

            # è·å–åŸå§‹æ£€æŸ¥ç»“æœ
            check_result = sentence_check_map.get(sentence_hash, {})
            r2_exists = check_result.get('r2_exists', False)
            cos_exists = check_result.get('cos_exists', False)

            # åªä¸Šä¼ åˆ°ç¼ºå¤±çš„å­˜å‚¨
            if not r2_exists:
                upload_files_r2.append(file_data)
            if not cos_exists:
                upload_files_cos.append(file_data)

    logger.info(f"å‡†å¤‡ä¸Šä¼ æ–‡ä»¶:")
    logger.info(f"   - R2 éœ€è¦ä¸Šä¼ : {len(upload_files_r2)} ä¸ª")
    logger.info(f"   - COS éœ€è¦ä¸Šä¼ : {len(upload_files_cos)} ä¸ª")

    upload_start = time.time()

    # åˆ†åˆ«ä¸Šä¼ åˆ° R2 å’Œ COSï¼ˆæ ¹æ®ç¼ºå¤±æƒ…å†µï¼‰
    if upload_files_r2 or upload_files_cos:
        cos_results, r2_results, cos_stats, r2_stats = await upload_audio_files(
            upload_files=[],  # ä½¿ç”¨ r2_files å’Œ cos_files æŒ‡å®šå„è‡ªçš„ä¸Šä¼ åˆ—è¡¨
            upload_to_cos=len(upload_files_cos) > 0,
            upload_to_r2=len(upload_files_r2) > 0,
            max_concurrent_r2=max_concurrent_r2,
            max_workers_cos=max_workers_cos,
            r2_files=upload_files_r2,  # åªä¸Šä¼ åˆ° R2 ç¼ºå¤±çš„æ–‡ä»¶
            cos_files=upload_files_cos  # åªä¸Šä¼ åˆ° COS ç¼ºå¤±çš„æ–‡ä»¶
        )
    else:
        logger.info("æ‰€æœ‰æ–‡ä»¶éƒ½å·²åœ¨R2å’ŒCOSå­˜åœ¨ï¼Œæ— éœ€ä¸Šä¼ ")
        cos_results, r2_results = [], []
        cos_stats = {'total_uploads': 0, 'successful_uploads': 0, 'failed_uploads': 0}
        r2_stats = {'total_uploads': 0, 'successful_uploads': 0, 'failed_uploads': 0}

    upload_time = time.time() - upload_start
    logger.info(f"âœ… ä¸Šä¼ å®Œæˆï¼Œè€—æ—¶: {upload_time:.1f}ç§’")

    total_time = audio_gen_time + upload_time

    stats = {
        'total': len(sentences),
        'generated': len(newly_generated),  # ä½¿ç”¨newly_generatedï¼Œæ›´å‡†ç¡®
        'local_existed': len(local_files) - len(newly_generated),  # æœ¬åœ°å·²å­˜åœ¨çš„æ•°é‡
        'uploaded_r2': r2_stats.get('successful_uploads', 0),
        'uploaded_cos': cos_stats.get('successful_uploads', 0),
        'r2_stats': r2_stats,
        'cos_stats': cos_stats,
        'performance': {
            'audio_generation_time': audio_gen_time,
            'upload_time': upload_time,
            'total_time': total_time,
            'audio_gen_rate': len(need_generate) / audio_gen_time if audio_gen_time > 0 else 0,
            'upload_rate': len(upload_files) / upload_time if upload_time > 0 else 0
        }
    }

    logger.info("=" * 60)
    logger.info("å®Œæˆç»Ÿè®¡ï¼š")
    logger.info(f"  - æ€»å¥å­æ•°: {stats['total']}")
    logger.info(f"  - æœ¬åœ°å·²å­˜åœ¨: {stats['local_existed']}")
    logger.info(f"  - æ–°ç”Ÿæˆ: {stats['generated']}")
    logger.info(f"  - R2 ä¸Šä¼ æˆåŠŸ: {stats['uploaded_r2']}")
    logger.info(f"  - COS ä¸Šä¼ æˆåŠŸ: {stats['uploaded_cos']}")
    logger.info("")
    logger.info("æ€§èƒ½ç»Ÿè®¡ï¼š")
    if audio_gen_time > 0:
        logger.info(f"  - éŸ³é¢‘ç”Ÿæˆè€—æ—¶: {audio_gen_time:.1f}ç§’ ({stats['performance']['audio_gen_rate']:.2f} å¥/ç§’)")
    logger.info(f"  - ä¸Šä¼ è€—æ—¶: {upload_time:.1f}ç§’ ({stats['performance']['upload_rate']:.2f} æ–‡ä»¶/ç§’)")
    logger.info(f"  - æ€»è€—æ—¶: {total_time:.1f}ç§’")
    logger.info("=" * 60)

    return stats


def group_by_episode(sentences: List[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:
    """
    æŒ‰ episode ID åˆ†ç»„å¥å­

    Args:
        sentences: å¥å­åˆ—è¡¨

    Returns:
        æŒ‰ episode_id åˆ†ç»„çš„å­—å…¸
    """
    episodes = {}
    for sentence in sentences:
        episode_id = sentence.get('episode_id')
        if episode_id not in episodes:
            episodes[episode_id] = []
        episodes[episode_id].append(sentence)

    return episodes


def format_check_results(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ ¼å¼åŒ–æ£€æŸ¥ç»“æœï¼ŒæŒ‰ episode åˆ†ç»„

    Args:
        all_results: æ‰€æœ‰å¥å­çš„æ£€æŸ¥ç»“æœ

    Returns:
        æ ¼å¼åŒ–åçš„ç»“æœå­—å…¸
    """
    # æŒ‰ episode åˆ†ç»„
    episodes = group_by_episode(all_results)

    # æ„å»ºç»“æœ
    formatted_results = {
        'total_sentences': len(all_results),
        'total_episodes': len(episodes),
        'episodes': {}
    }

    for episode_id in sorted(episodes.keys()):
        episode_sentences = episodes[episode_id]

        # ç»Ÿè®¡è¯¥ episode çš„éŸ³é¢‘çŠ¶æ€
        audio_exists_count = sum(1 for s in episode_sentences if s.get('audio_exists', False))
        r2_exists_count = sum(1 for s in episode_sentences if s.get('r2_exists', False))
        cos_exists_count = sum(1 for s in episode_sentences if s.get('cos_exists', False))

        formatted_results['episodes'][f'EP{episode_id}'] = {
            'episode_id': episode_id,
            'total_sentences': len(episode_sentences),
            'audio_exists_count': audio_exists_count,
            'audio_missing_count': len(episode_sentences) - audio_exists_count,
            'r2_exists_count': r2_exists_count,
            'cos_exists_count': cos_exists_count,
            'sentences': [
                {
                    'sentence_id': s.get('sentence_id'),
                    'episode_sequence': s.get('episode_sequence'),
                    'en': s.get('en'),
                    'sentence_hash': s.get('sentence_hash'),
                    'r2_exists': s.get('r2_exists', False),
                    'cos_exists': s.get('cos_exists', False),
                    'audio_exists': s.get('audio_exists', False)
                }
                for s in sorted(episode_sentences, key=lambda x: x.get('episode_sequence', 0))
            ]
        }

    return formatted_results


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æ£€æŸ¥å¹¶ç”ŸæˆæŒ‡å®š Episode èŒƒå›´çš„éŸ³é¢‘æ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s -s 238 -e 300                    # å¤„ç† Episode 238-300
  %(prog)s --start 1 --end 100               # å¤„ç† Episode 1-100
  %(prog)s -s 50 -e 60 --checks 100          # è‡ªå®šä¹‰æ£€æŸ¥å¹¶å‘æ•°
  %(prog)s -s 1 -e 10 --audio-workers 10     # è‡ªå®šä¹‰éŸ³é¢‘ç”Ÿæˆå¹¶å‘æ•°

æ€§èƒ½å‚æ•°:
  é»˜è®¤é…ç½®å·²é’ˆå¯¹ä¸€èˆ¬åœºæ™¯ä¼˜åŒ–ï¼Œé€šå¸¸æ— éœ€ä¿®æ”¹
  å¦‚éœ€è°ƒä¼˜ï¼Œå¯æ ¹æ®æœåŠ¡å™¨æ€§èƒ½å’Œç½‘ç»œæƒ…å†µè°ƒæ•´å„é¡¹å¹¶å‘å‚æ•°
        """
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument(
        '-s', '--start',
        type=int,
        required=True,
        help='èµ·å§‹ Episode IDï¼ˆå¿…éœ€ï¼‰'
    )

    parser.add_argument(
        '-e', '--end',
        type=int,
        required=True,
        help='ç»“æŸ Episode IDï¼ˆå¿…éœ€ï¼‰'
    )

    # å¯é€‰çš„æ€§èƒ½å‚æ•°
    parser.add_argument(
        '--checks',
        type=int,
        default=4,
        help='æ–‡ä»¶æ£€æŸ¥å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š4ï¼‰'
    )

    parser.add_argument(
        '--audio-workers',
        type=int,
        default=3,
        help='éŸ³é¢‘ç”Ÿæˆå¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰'
    )

    parser.add_argument(
        '--r2-workers',
        type=int,
        default=2,
        help='R2 ä¸Šä¼ å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰'
    )

    parser.add_argument(
        '--cos-workers',
        type=int,
        default=3,
        help='COS ä¸Šä¼ çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰'
    )

    parser.add_argument(
        '--data-file',
        type=str,
        default='prod_lingohow-sentences-20251113.json',
        help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šprod_lingohow-sentences-20251113.jsonï¼‰'
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.start < 1:
        parser.error("èµ·å§‹ Episode ID å¿…é¡»å¤§äºç­‰äº 1")

    if args.end < args.start:
        parser.error(f"ç»“æŸ Episode ID ({args.end}) ä¸èƒ½å°äºèµ·å§‹ Episode ID ({args.start})")

    return args


async def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # ä»å‚æ•°è·å–é…ç½®
    START_EPISODE = args.start
    END_EPISODE = args.end
    MAX_CONCURRENT_CHECKS = args.checks
    MAX_CONCURRENT_AUDIO = args.audio_workers
    MAX_CONCURRENT_R2 = args.r2_workers
    MAX_WORKERS_COS = args.cos_workers
    data_file_path = args.data_file

    # è¯»å–æ•°æ®æ–‡ä»¶
    data_file = Path(data_file_path)

    if not data_file.exists():
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return

    logger.info("=" * 60)
    logger.info(f"ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶: {data_file}")
    with open(data_file, 'r', encoding='utf-8') as f:
        all_sentences = json.load(f)

    logger.info(f"   æ€»å¥å­æ•°: {len(all_sentences)}")

    # ç­›é€‰æŒ‡å®šèŒƒå›´çš„ episode å¥å­
    filtered_sentences = [
        s for s in all_sentences
        if START_EPISODE <= s.get('episode_id', 0) <= END_EPISODE
    ]

    logger.info(f"   ç­›é€‰èŒƒå›´: Episode {START_EPISODE} åˆ° Episode {END_EPISODE}")
    logger.info(f"   ç­›é€‰åå¥å­æ•°: {len(filtered_sentences)}")

    logger.info("")
    logger.info("âš™ï¸  æ€§èƒ½é…ç½®:")
    logger.info(f"   - æ–‡ä»¶æ£€æŸ¥å¹¶å‘æ•°: {MAX_CONCURRENT_CHECKS}")
    logger.info(f"   - éŸ³é¢‘ç”Ÿæˆå¹¶å‘æ•°: {MAX_CONCURRENT_AUDIO}")
    logger.info(f"   - R2 ä¸Šä¼ å¹¶å‘æ•°: {MAX_CONCURRENT_R2}")
    logger.info(f"   - COS ä¸Šä¼ çº¿ç¨‹æ•°: {MAX_WORKERS_COS}")
    logger.info("=" * 60)

    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    all_results, missing_sentences = await check_all_sentences(
        filtered_sentences,
        max_concurrent_checks=MAX_CONCURRENT_CHECKS
    )

    # æ ¼å¼åŒ–å¹¶ä¿å­˜å®Œæ•´çš„æ£€æŸ¥ç»“æœï¼ˆæŒ‰ episode åˆ†ç»„ï¼‰
    formatted_results = format_check_results(all_results)

    # ä¿å­˜å®Œæ•´æ£€æŸ¥ç»“æœ
    full_results_file = Path(
        f"audio_check_results_ep{START_EPISODE}-{END_EPISODE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    with open(full_results_file, 'w', encoding='utf-8') as f:
        json.dump(formatted_results, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ“Š å®Œæ•´æ£€æŸ¥ç»“æœå·²ä¿å­˜åˆ°: {full_results_file}")

    # åŒæ—¶ä¿å­˜ç¼ºå¤±éŸ³é¢‘çš„å¥å­åˆ—è¡¨ï¼ˆç”¨äºç”Ÿæˆï¼‰
    if missing_sentences:
        missing_file = Path(
            f"missing_audio_ep{START_EPISODE}-{END_EPISODE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(missing_file, 'w', encoding='utf-8') as f:
            json.dump(missing_sentences, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ç¼ºå¤±éŸ³é¢‘çš„å¥å­å·²ä¿å­˜åˆ°: {missing_file}")

    # æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    logger.info("")
    logger.info("=" * 60)
    logger.info("ğŸ“Š æ£€æŸ¥ç»“æœç»Ÿè®¡")
    logger.info("=" * 60)

    # æ€»ä½“ç»Ÿè®¡
    total_sentences = len(all_results)
    audio_exists = total_sentences - len(missing_sentences)

    logger.info(f"æ€»å¥å­æ•°: {total_sentences}")
    logger.info(f"  - éŸ³é¢‘å·²å­˜åœ¨: {audio_exists} ({audio_exists * 100 / total_sentences:.1f}%)")
    logger.info(f"  - éŸ³é¢‘ç¼ºå¤±: {len(missing_sentences)} ({len(missing_sentences) * 100 / total_sentences:.1f}%)")

    # ç¼ºå¤±è¯¦æƒ…
    if missing_sentences:
        r2_missing = sum(1 for s in missing_sentences if not s.get('r2_exists', False))
        cos_missing = sum(1 for s in missing_sentences if not s.get('cos_exists', False))
        both_missing = sum(1 for s in missing_sentences if not s.get('r2_exists', False) and not s.get('cos_exists', False))

        logger.info(f"\nç¼ºå¤±è¯¦æƒ…:")
        logger.info(f"  - R2 ç¼ºå¤±: {r2_missing}")
        logger.info(f"  - COS ç¼ºå¤±: {cos_missing}")
        logger.info(f"  - ä¸¤è€…éƒ½ç¼ºå¤±: {both_missing}")

        # æŒ‰ episode æ˜¾ç¤ºç¼ºå¤±ç»Ÿè®¡
        missing_by_episode = group_by_episode(missing_sentences)
        logger.info(f"\næŒ‰ Episode åˆ†å¸ƒ:")
        for episode_id in sorted(missing_by_episode.keys()):
            count = len(missing_by_episode[episode_id])
            logger.info(f"  - EP{episode_id}: {count} ä¸ªå¥å­ç¼ºå¤±éŸ³é¢‘")

    logger.info("=" * 60)

    # è¯¢é—®æ˜¯å¦ç”ŸæˆéŸ³é¢‘
    if missing_sentences:
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸµ æ˜¯å¦ç»§ç»­ç”Ÿæˆå¹¶ä¸Šä¼ ç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶ï¼Ÿ")
        logger.info(f"   å°†ç”Ÿæˆ {len(missing_sentences)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        logger.info(f"   èŒƒå›´: Episode {START_EPISODE} åˆ° Episode {END_EPISODE}")
        logger.info("   æŒ‰ Ctrl+C å–æ¶ˆï¼Œæˆ–ç­‰å¾… 10 ç§’è‡ªåŠ¨ç»§ç»­...")
        logger.info("=" * 60)

        try:
            await asyncio.sleep(10)
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return

        # ç”Ÿæˆå¹¶ä¸Šä¼ éŸ³é¢‘
        stats = await generate_and_upload_audio(
            missing_sentences,
            max_concurrent_audio=MAX_CONCURRENT_AUDIO,
            max_concurrent_r2=MAX_CONCURRENT_R2,
            max_workers_cos=MAX_WORKERS_COS
        )

        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_file = Path(
            f"audio_stats_ep{START_EPISODE}-{END_EPISODE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        # æ·»åŠ èŒƒå›´ä¿¡æ¯åˆ°ç»Ÿè®¡
        stats['episode_range'] = {
            'start': START_EPISODE,
            'end': END_EPISODE,
            'total_episodes': END_EPISODE - START_EPISODE + 1
        }

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_file}")
        logger.info(f"âœ… æ‰€æœ‰æ“ä½œå®Œæˆï¼(Episode {START_EPISODE}-{END_EPISODE})")
    else:
        logger.info(f"âœ… Episode {START_EPISODE}-{END_EPISODE} çš„æ‰€æœ‰å¥å­éŸ³é¢‘æ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€ç”Ÿæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
